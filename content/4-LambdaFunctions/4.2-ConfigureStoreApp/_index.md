---
title : "Configure and test StoreApp"
date : "`r Sys.Date()`"
weight : 2
chapter : false
pre : " <b> 4.2 </b> "
---

In this step, you will configure IAM permissions for the StoreApp Lambda function to send data to Kinesis Data Firehose, then test the function to verify that data is successfully streamed to the S3 bucket.

## Why Configure Permissions?

The Lambda function needs specific permissions to:

- **Access Kinesis Data Firehose**: Send inventory records to the SI-Firehose stream
- **Write to CloudWatch Logs**: Log execution details for troubleshooting
- **Security Best Practice**: Follow the principle of least privilege

### Permission Model
AWS Lambda automatically creates an execution role when creating a function, but we need to add Firehose permissions so the function can stream data.

## Step-by-Step Instructions

### Step 1: Create Test Event

1. Return to the **"StoreApp"** Lambda function
2. Click the **"Test"** tab at the top
3. Select **"Create new event"**
4. **Event name**: Name it `test`
5. Keep the default template
6. Click **"Save"** to complete

![Create Test Event](/images/15.png)

{{% notice info %}}
**Test Event**: Allows you to manually trigger the Lambda function to test functionality before integrating with other services.
{{% /notice %}}

### Step 2: Navigate to IAM Roles

1. For Lambda to work, we need to add permissions to the IAM role
2. Open a new tab, go to **IAM Console**
3. Go to **"Roles"** section
4. Search **"store"** to display the IAM role for the "StoreApp" lambda
5. Select that role (usually named like `StoreApp-role-xxxxx`)

![Navigate to IAM Roles](/images/15_1.png)

### Step 3: Add Firehose Permissions

1. In the IAM role page, find the **"Add permissions"** section
2. Select **"Attach policies"**

![Add Permissions](/images/15_2.png)

### Step 4: Attach Firehose Policy

1. In the permissions search bar, type **"firehose"**
2. Find and check **"AmazonKinesisFirehoseFullAccess"**
3. Click **"Add permissions"** to attach the policy

![Attach Firehose Policy](/images/15_3.png)

{{% notice warning %}}
**Security Note**: In production, you should use a custom policy with minimal permissions instead of FullAccess. This workshop uses FullAccess for simplicity.
{{% /notice %}}

### Step 5: Test Lambda Function

1. Return to Lambda **"StoreApp"**
2. Go to the **"Test"** tab
3. Click the **"Test"** button to execute the test event

![Test Lambda Function](/images/16.png)

### Step 6: Review Test Results

1. After successfully executing the test event
2. Detailed logs will display the execution results
3. Check for **"Succeeded"** status and review logs to ensure data was sent

![Review Test Results](/images/17.png)

{{% notice success %}}
**Test Successful**: If you see "Succeeded" status with no error logs, the Lambda function has successfully sent data to Firehose.
{{% /notice %}}

### Step 7: Verify Data in S3 - Access Bucket

1. Open a new tab, access the **S3 Console**
2. Go to the **consumption-bucket** created at the beginning of the workshop

![Access S3 Bucket](/images/18.png)

### Step 8: Navigate to Bucket Contents

1. Click on the consumption bucket to view contents

![Navigate Bucket Contents](/images/19.png)

### Step 9: Refresh and Check Objects

1. **Refresh** the page to update the latest objects
2. You should see the **store-data** folder created by Firehose

![Refresh and Check Objects](/images/20.png)

{{% notice tip %}}
**Dynamic Partitioning**: Firehose automatically creates folder structure according to the configured pattern: `store-data/store_id=XXX/year/month/day/hour/`
{{% /notice %}}

### Step 10: Verify Firehose Output Files

1. Navigate to the partitioned folder path
2. You should see SI-Firehose output files (Parquet format)
3. This is the inventory data that has been converted and stored in the data lake

![Verify Firehose Output](/images/21.png)

## Verification Checklist

Confirm the following conditions have been completed:

✅ **IAM Permissions**: StoreApp role has AmazonKinesisFirehoseFullAccess  
✅ **Test Event**: Created and executed successfully  
✅ **Lambda Logs**: Status "Succeeded" with no errors  
✅ **S3 Data**: Files appear in consumption bucket  
✅ **Data Structure**: Folders created according to dynamic partitioning pattern  
✅ **File Format**: Parquet files generated by Firehose  

{{% notice success %}}
**End-to-End Test Successful!** Inventory data has been successfully streamed from StoreApp → Firehose → S3. The basic pipeline is working!
{{% /notice %}}

{{% notice info %}}
**What Happened**: StoreApp generated inventory data → sent to SI-Firehose → converted to Parquet → stored in S3 with dynamic partitioning → ready for Athena analytics!
{{% /notice %}}
