[
{
	"uri": "//localhost:1313/2-prerequiste/2.1-creates3bucket/",
	"title": "Create S3 Bucket",
	"tags": [],
	"description": "",
	"content": "In this step, you will create an S3 bucket that will serve as the consumption zone for your streaming inventory data. This bucket will act as a data lake where all inventory records from your stores will be stored and made available for analytics.\nWhy Do We Need This Bucket? The S3 consumption bucket serves multiple purposes in our streaming analytics pipeline:\nData Lake Storage: Centralized repository for all streaming inventory data Athena Data Source: Amazon Athena will query data stored in this bucket Firehose Destination: Kinesis Data Firehose will deliver streaming records here Cost-Effective Storage: S3 provides durable, scalable storage at low cost Step-by-Step Instructions Step 1: Navigate to S3 Console From the AWS Management Console home page, locate the search bar at the top Type \u0026ldquo;s3\u0026rdquo; in the search field Click on Amazon S3 from the search results Step 2: Access Bucket Creation Click the \u0026ldquo;Create bucket\u0026rdquo; button to start the bucket creation process Step 3: Configure Bucket Settings In the Bucket name field, enter: consumption-bucket-yourname\nImportant: Replace \u0026ldquo;yourname\u0026rdquo; with your actual name or a unique identifier (e.g., consumption-bucket-nguyenvana). S3 bucket names must be globally unique across all AWS accounts worldwide.\nRegion: Ensure you\u0026rsquo;re creating the bucket in Singapore / ap-southeast-1\nLeave all other settings as default (we\u0026rsquo;ll use the recommended security settings)\nScroll down to the bottom of the page\nClick \u0026ldquo;Create bucket\u0026rdquo; to complete the creation\nVerification After successful creation, you should see your new bucket listed in the S3 console with the name consumption-bucket-yourname.\nPro Tip: Take note of your exact bucket name. You\u0026rsquo;ll need this when configuring Kinesis Data Firehose in the next modules.\nBucket Naming Rules: S3 bucket names must be 3-63 characters long, contain only lowercase letters, numbers, and hyphens, and cannot start or end with a hyphen.\n"
},
{
	"uri": "//localhost:1313/4-lambdafunctions/4.1-storeapp/",
	"title": "Create StoreApp Function",
	"tags": [],
	"description": "",
	"content": "In this step, you will create the StoreApp Lambda function - an application that simulates stores sending real-time inventory data to the Kinesis Data Firehose stream. This Lambda function will generate and stream store inventory data to serve the analytics pipeline.\nStoreApp Lambda Function Overview The StoreApp Lambda function plays a crucial role in the streaming analytics architecture:\nData Generator: Creates simulated inventory data from multiple different stores Streaming Integration: Sends data directly to Kinesis Data Firehose Event-Driven: Can be triggered manually or on a schedule JSON Format: Generates data with JSON structure compatible with Glue table schema Step-by-Step Instructions Step 1: Navigate to Lambda Console In the AWS Console search bar Type \u0026ldquo;lambda\u0026rdquo; Select \u0026ldquo;Lambda\u0026rdquo; from the search results Step 2: Create New Function Click the \u0026ldquo;Create function\u0026rdquo; button in the top right corner Step 3: Configure Function Settings Function name: Name it StoreApp Runtime: Select \u0026ldquo;Python 3.13\u0026rdquo; Leave other settings as default Click \u0026ldquo;Create function\u0026rdquo; to proceed to the next step Python 3.13: Latest version with improved performance and better support for AWS SDK (boto3).\nStep 4: Edit Runtime Settings After creating the function, in the \u0026ldquo;Code\u0026rdquo; section Scroll down to \u0026ldquo;Runtime settings\u0026rdquo; Click \u0026ldquo;Edit\u0026rdquo; Step 5: Update Handler Configuration In the new interface that appears: Runtime: Select \u0026ldquo;Python 3.13\u0026rdquo; Handler: Enter function.lambda_handler Click \u0026ldquo;Save\u0026rdquo; to save the configuration Handler: function.lambda_handler means Lambda will call the lambda_handler() function in the function.py file.\nStep 6: Create Function Code File In the \u0026ldquo;Code\u0026rdquo; section, find and click the new file creation icon (highlighted in the image) Name the new file: function.py Copy content from the following link and paste into the file: üìÅ storeapp-function.py Click the \u0026ldquo;Deploy\u0026rdquo; button to deploy the code üìÅ Source Code Required: Copy content from the link storeapp-function.py and paste into the function.py file in the Lambda editor.\nStep 7: Configure Environment Variables Switch to the \u0026ldquo;Configuration\u0026rdquo; tab In the sidebar, select \u0026ldquo;Environment variables\u0026rdquo; Click \u0026ldquo;Edit\u0026rdquo; Step 8: Add Firehose Stream Variable In the environment variables interface: Click \u0026ldquo;Add environment variable\u0026rdquo; Key: delivery_stream Value: SI-Firehose (name of the Firehose stream created in step 3) Click \u0026ldquo;Save\u0026rdquo; to complete Environment Variables: Allows the Lambda function to know the Firehose stream name for sending data without hardcoding it in the source code.\nVerification After completing the above steps, your StoreApp Lambda function has been configured with:\n‚úÖ Runtime: Python 3.13\n‚úÖ Handler: function.lambda_handler\n‚úÖ Source Code: Deployed successfully\n‚úÖ Environment Variable: delivery_stream = SI-Firehose\nPart 1 Complete! StoreApp Lambda function has been successfully created and configured. In the next step, we\u0026rsquo;ll configure permissions and test the function.\n"
},
{
	"uri": "//localhost:1313/1-introduce/",
	"title": "Introduction",
	"tags": [],
	"description": "",
	"content": "Streaming Analytics is an AWS solution for collecting, storing, and processing real-time data from various sources. In this workshop, we will apply it to the scenario of store inventory management and delivery coordination.\nEach store sends its product inventory data through the StoreApp into Amazon Kinesis Data Firehose, which streams the data to an S3 bucket for storage. The user will then manually invoke a Lambda function (StorePlanningApp) to query the data using Amazon Athena, analyze inventory levels, and detect which stores are running low on specific products. The results will then be sent through Amazon SQS to notify the StoreTruckApp, which coordinates delivery trucks for restocking.\nAdvantages of this approach Compared to traditional inventory management methods, AWS Streaming Analytics provides several benefits:\nReal-time inventory monitoring with data lake storage instead of waiting for periodic reports Serverless Lambda processing with Athena analytics, eliminating the need to manage servers or databases Scalable data ingestion via Kinesis Data Firehose, able to handle data from hundreds of stores simultaneously Reliable message queuing with SQS, ensuring delivery coordination messages are not lost Cost-effective analytics, you only pay for the data processed and queries executed Flexible data storage in S3, enabling historical analysis and machine learning capabilities Architecture Components Integration StoreApp: Collects and streams inventory data from retail locations Kinesis Data Firehose: Reliably ingests streaming data to S3 with automatic scaling S3 Consumption Zone: Serves as a data lake for inventory records Amazon Athena: Provides serverless SQL analytics on S3 data StorePlanningApp (Lambda): Processes inventory analytics and triggers alerts Amazon SQS: Manages delivery coordination message queue StoreTruckApp: Monitors SQS and dispatches delivery trucks With these integrated components, the Streaming Analytics system enables more efficient supply chain management, reduces the risk of stock shortages, improves delivery coordination, and enhances overall customer satisfaction through better inventory availability.\n"
},
{
	"uri": "//localhost:1313/4-lambdafunctions/4.2-configurestoreapp/",
	"title": "Configure and test StoreApp",
	"tags": [],
	"description": "",
	"content": "In this step, you will configure IAM permissions for the StoreApp Lambda function to send data to Kinesis Data Firehose, then test the function to verify that data is successfully streamed to the S3 bucket.\nWhy Configure Permissions? The Lambda function needs specific permissions to:\nAccess Kinesis Data Firehose: Send inventory records to the SI-Firehose stream Write to CloudWatch Logs: Log execution details for troubleshooting Security Best Practice: Follow the principle of least privilege Permission Model AWS Lambda automatically creates an execution role when creating a function, but we need to add Firehose permissions so the function can stream data.\nStep-by-Step Instructions Step 1: Create Test Event Return to the \u0026ldquo;StoreApp\u0026rdquo; Lambda function Click the \u0026ldquo;Test\u0026rdquo; tab at the top Select \u0026ldquo;Create new event\u0026rdquo; Event name: Name it test Keep the default template Click \u0026ldquo;Save\u0026rdquo; to complete Test Event: Allows you to manually trigger the Lambda function to test functionality before integrating with other services.\nStep 2: Navigate to IAM Roles For Lambda to work, we need to add permissions to the IAM role Open a new tab, go to IAM Console Go to \u0026ldquo;Roles\u0026rdquo; section Search \u0026ldquo;store\u0026rdquo; to display the IAM role for the \u0026ldquo;StoreApp\u0026rdquo; lambda Select that role (usually named like StoreApp-role-xxxxx) Step 3: Add Firehose Permissions In the IAM role page, find the \u0026ldquo;Add permissions\u0026rdquo; section Select \u0026ldquo;Attach policies\u0026rdquo; Step 4: Attach Firehose Policy In the permissions search bar, type \u0026ldquo;firehose\u0026rdquo; Find and check \u0026ldquo;AmazonKinesisFirehoseFullAccess\u0026rdquo; Click \u0026ldquo;Add permissions\u0026rdquo; to attach the policy Security Note: In production, you should use a custom policy with minimal permissions instead of FullAccess. This workshop uses FullAccess for simplicity.\nStep 5: Test Lambda Function Return to Lambda \u0026ldquo;StoreApp\u0026rdquo; Go to the \u0026ldquo;Test\u0026rdquo; tab Click the \u0026ldquo;Test\u0026rdquo; button to execute the test event Step 6: Review Test Results After successfully executing the test event Detailed logs will display the execution results Check for \u0026ldquo;Succeeded\u0026rdquo; status and review logs to ensure data was sent Test Successful: If you see \u0026ldquo;Succeeded\u0026rdquo; status with no error logs, the Lambda function has successfully sent data to Firehose.\nStep 7: Verify Data in S3 - Access Bucket Open a new tab, access the S3 Console Go to the consumption-bucket created at the beginning of the workshop Step 8: Navigate to Bucket Contents Click on the consumption bucket to view contents Step 9: Refresh and Check Objects Refresh the page to update the latest objects You should see the store-data folder created by Firehose Dynamic Partitioning: Firehose automatically creates folder structure according to the configured pattern: store-data/store_id=XXX/year/month/day/hour/\nStep 10: Verify Firehose Output Files Navigate to the partitioned folder path You should see SI-Firehose output files (Parquet format) This is the inventory data that has been converted and stored in the data lake Verification Checklist Confirm the following conditions have been completed:\n‚úÖ IAM Permissions: StoreApp role has AmazonKinesisFirehoseFullAccess\n‚úÖ Test Event: Created and executed successfully\n‚úÖ Lambda Logs: Status \u0026ldquo;Succeeded\u0026rdquo; with no errors\n‚úÖ S3 Data: Files appear in consumption bucket\n‚úÖ Data Structure: Folders created according to dynamic partitioning pattern\n‚úÖ File Format: Parquet files generated by Firehose\nEnd-to-End Test Successful! Inventory data has been successfully streamed from StoreApp ‚Üí Firehose ‚Üí S3. The basic pipeline is working!\nWhat Happened: StoreApp generated inventory data ‚Üí sent to SI-Firehose ‚Üí converted to Parquet ‚Üí stored in S3 with dynamic partitioning ‚Üí ready for Athena analytics!\n"
},
{
	"uri": "//localhost:1313/2-prerequiste/2.2-createtableanddatabase/",
	"title": "Create Database &amp; Table",
	"tags": [],
	"description": "",
	"content": "In this step, you will create an AWS Glue database and table that will serve as the metadata catalog for your streaming inventory data. This catalog enables Amazon Athena to understand and query the data stored in your S3 consumption bucket.\nOverview AWS Glue Data Catalog acts as a centralized metadata repository that stores information about your data\u0026rsquo;s structure, format, and location. When you create a database and table in Glue, you\u0026rsquo;re essentially creating a schema that Athena can use to interpret your raw data files.\nWhat we\u0026rsquo;ll create:\nDatabase: conversion_db - Logical container for our inventory tables Table: conversion_table - Schema definition for inventory data structure Part A: Creating the Glue Database Step 1: Navigate to AWS Glue Console From the AWS Management Console home page, locate the search bar at the top Type \u0026ldquo;Glue\u0026rdquo; in the search field Click on \u0026ldquo;AWS Glue\u0026rdquo; from the search results Step 2: Access Databases Section In the AWS Glue console, look at the left navigation panel Find and click on \u0026ldquo;Databases\u0026rdquo; under the Data Catalog section In the Databases page, click the \u0026ldquo;Add database\u0026rdquo; button located in the top-right corner Step 3: Create the Database In the Database name field, enter: conversion_db Leave other settings as default Click \u0026ldquo;Create database\u0026rdquo; to complete the creation Success! You have successfully created the conversion_db database. This database will serve as the logical container for our inventory data tables.\nPart B: Creating the Glue Table Step 4: Navigate to Tables Section In the left navigation panel, under \u0026ldquo;Databases\u0026rdquo;, click on \u0026ldquo;Tables\u0026rdquo; Click the \u0026ldquo;Add table\u0026rdquo; button in the top-right corner Step 5: Configure Table Basic Information Table name: Enter conversion_table Database: Select conversion_db from the dropdown (the database we just created) Leave other basic settings as default Step 6: Configure Data Store Scroll down to the \u0026ldquo;Include path\u0026rdquo; section\nClick \u0026ldquo;Browse S3\u0026rdquo; or manually enter the path to your S3 bucket:\ns3://consumption-bucket-yourname/ Replace \u0026ldquo;yourname\u0026rdquo; with the actual name you used when creating your S3 bucket in step 2.1\nIn the \u0026ldquo;Data format\u0026rdquo; section, select \u0026ldquo;Parquet\u0026rdquo;\nClick \u0026ldquo;Next\u0026rdquo; to proceed\nStep 7: Define Schema Using JSON On the schema definition page, select Edit schema as JSON Copy the schema content from this link: üìÅ inventory-schema.json Paste the copied JSON content into the Edit schema as JSON field Click \u0026ldquo;Save\u0026rdquo; to apply the schema Schema Content Required: You must copy the exact JSON schema from the provided link above. This pre-defined schema matches the inventory data structure that will be streamed from your StoreApp. Do not modify the schema content as it needs to match the incoming data format.\nStep 8: Finalize Table Creation Review your table configuration Click \u0026ldquo;Next\u0026rdquo; to proceed to the final step Review the summary and click \u0026ldquo;Create table\u0026rdquo; to complete Verification After successful creation, you should see:\nDatabase: conversion_db in the Databases list Table: conversion_table under the conversion_db database Congratulations! You have successfully created both the Glue database and table. Amazon Athena can now use this metadata catalog to query your streaming inventory data.\n"
},
{
	"uri": "//localhost:1313/2-prerequiste/",
	"title": "Preparation",
	"tags": [],
	"description": "",
	"content": "In this module, you will prepare the foundational infrastructure components needed for our streaming analytics pipeline. These components will serve as the backbone for data storage, processing, and analysis throughout the workshop.\nInfrastructure Components Overview Before we can start streaming and analyzing store inventory data, we need to set up:\nS3 Consumption Bucket - Data lake storage for streamed inventory records IAM Role for Kinesis Data Firehose - Permissions for data delivery to S3 Conversion Database - AWS Glue Data Catalog database for organizing our data Conversion Table - Glue table structure that Athena will use for querying inventory data Why These Components Matter S3 Consumption Bucket Acts as your data lake where all streaming inventory data from stores will be stored. This bucket serves as the central repository that Amazon Athena will query for analytics.\nIAM Role for Firehose Ensures secure data delivery from Kinesis Data Firehose to your S3 bucket. Without proper permissions, the streaming pipeline cannot function.\nConversion Database \u0026amp; Table Creates the metadata catalog in AWS Glue Data Catalog that defines the schema for your inventory data. Amazon Athena uses this Glue catalog to understand the structure of your S3 data, enabling SQL queries on raw JSON inventory records. The database serves as a logical container, while the table definition maps JSON fields to queryable columns.\nPreparation Time: This module should take approximately 15-20 minutes to complete all setup tasks.\nImportant: Complete all preparation steps in order. Each component depends on the previous ones being properly configured.\nWorkshop Content Create S3 Consumption Bucket Create IAM Role for Kinesis Data Firehose Create Conversion Database in AWS Glue Data Catalog Create Conversion Table in AWS Glue Data Catalog Once you complete this preparation phase, you\u0026rsquo;ll have a solid foundation to build your streaming analytics pipeline for store inventory management.\n"
},
{
	"uri": "//localhost:1313/",
	"title": "Streaming Analytics for Store Inventory Management",
	"tags": [],
	"description": "",
	"content": "Introduction In this lab, you will build a streaming analytics system to monitor store inventory levels and coordinate delivery truck dispatching for restocking.\nStore product data is ingested through the StoreApp into Amazon Kinesis Data Firehose, which streams the data to an S3 bucket. You will manually invoke a Lambda function to query the data using Amazon Athena, analyze inventory levels, and detect which stores need restocking for specific products.\nOnce the Lambda function is executed, low-inventory alerts will be sent to the Amazon SQS Queue, which the StoreTruckApp monitors to dispatch delivery trucks to the appropriate store locations.\nLearning Objectives Understand how to ingest real-time store data using Amazon Kinesis Data Firehose Practice creating and manually triggering Lambda functions to process inventory analytics Query streaming data stored in S3 using Amazon Athena Send delivery coordination messages via Amazon SQS Build an end-to-end streaming analytics pipeline for retail operations Overall Architecture Product data ‚Üí StoreApp streams to Amazon Kinesis Data Firehose Kinesis Data Firehose ‚Üí stores data in S3 bucket (consumption zone) Amazon Athena ‚Üí queries S3 data for inventory analysis User manually invokes Lambda ‚Üí StorePlanningApp processes inventory levels Lambda ‚Üí detects stores with low inventory and sends alerts to Amazon SQS Queue StoreTruckApp ‚Üí consumes SQS messages and coordinates Delivery trucks Delivery trucks ‚Üí restock stores based on inventory needs Key Point: Unlike fully automated systems, you will manually trigger the Lambda function to process inventory data and coordinate deliveries. This gives you hands-on control over the analytics pipeline.\n"
},
{
	"uri": "//localhost:1313/2-prerequiste/2.3-createfirehoseiamrole/",
	"title": "Create IAM Role for Kinesis Data Firehose",
	"tags": [],
	"description": "",
	"content": "In this step, you will create an IAM Role that allows Kinesis Data Firehose to access your S3 bucket and AWS Glue Data Catalog for writing streaming inventory data.\nWhy Do We Need an IAM Role? The IAM Role provides necessary permissions for Kinesis Data Firehose to:\nWrite data to S3: Store streaming data in the consumption bucket Access Glue Data Catalog: Update table metadata and schema Security: Follow AWS \u0026ldquo;least privilege\u0026rdquo; security best practices Step-by-Step Instructions Step 1: Navigate to IAM Console From the AWS Management Console home page, locate the search bar at the top Type \u0026ldquo;iam\u0026rdquo; in the search field Under \u0026ldquo;IAM Features\u0026rdquo;, click on \u0026ldquo;Roles\u0026rdquo; Step 2: Create New Role In the top-right corner of the IAM Roles page Click the \u0026ldquo;Create role\u0026rdquo; button Step 3: Configure Trusted Entity Trusted entity type: Keep the default \u0026ldquo;AWS service\u0026rdquo; Under \u0026ldquo;Use case\u0026rdquo;, find and select \u0026ldquo;Firehose\u0026rdquo; from the list Click \u0026ldquo;Next\u0026rdquo; Step 4: Add Permissions We will add 2 main roles for 2 services: Search and add \u0026ldquo;AmazonS3FullAccess\u0026rdquo; Search and add \u0026ldquo;AWSGlueServiceRole\u0026rdquo; Click \u0026ldquo;Next\u0026rdquo; Security Note: In this workshop, we use FullAccess permissions for learning convenience. However, in real-world production environments, you should NEVER use FullAccess permissions. Instead, apply the \u0026ldquo;Least Privilege\u0026rdquo; principle - only grant the minimum permissions necessary for each specific task.\nStep 5: Name and Create Role We will name this role \u0026ldquo;FirehoseWorkshop\u0026rdquo; Review the \u0026ldquo;Permissions\u0026rdquo; section has 2 main services added: AmazonS3FullAccess AWSGlueServiceRole Click \u0026ldquo;Create role\u0026rdquo; Verification After successful creation, you should see the FirehoseWorkshop role in the list with trusted entity firehose.amazonaws.com and 2 permissions attached.\nComplete! IAM Role for Kinesis Data Firehose has been successfully created and is ready to use.\n"
},
{
	"uri": "//localhost:1313/4-lambdafunctions/4.3-createsqs/",
	"title": "Create SQS Queue",
	"tags": [],
	"description": "",
	"content": "In this step, you will create an Amazon SQS Queue - a message queue to handle planning requests from the store system. The SQS Queue will serve as a buffer between components in the streaming analytics architecture.\nAmazon SQS Overview Amazon Simple Queue Service (SQS) is a fully managed message queuing service that enables you to decouple and scale microservices, distributed systems, and serverless applications.\nSQS Role in Our System In our Store Planning architecture, the SQS Queue will:\nMessage Buffer: Temporarily store messages from the system Decoupling: Separate data producers from consumers Reliability: Ensure messages are not lost during processing Scalability: Automatically scale with message volume SQS in Streaming Pipeline: The queue will receive messages from various sources and trigger Lambda functions to process planning logic.\nStep-by-Step Instructions Step 1: Navigate to SQS Console In the AWS Console search bar Type \u0026ldquo;sqs\u0026rdquo; Select \u0026ldquo;Simple Queue Service\u0026rdquo; from the search results Step 2: Create New Queue In the SQS Console interface Click the \u0026ldquo;Create queue\u0026rdquo; button Step 3: Configure Queue Settings Queue name: Name it Store_Queue Queue type: Keep default \u0026ldquo;Standard\u0026rdquo; Leave other configurations as default settings Click \u0026ldquo;Create queue\u0026rdquo; to complete Standard vs FIFO Queue: Standard queue provides high throughput and at-least-once delivery, suitable for our streaming analytics use case.\nVerify Queue Creation After successful creation, you should see:\n‚úÖ Queue Name: Store_Queue\n‚úÖ Queue Type: Standard\n‚úÖ Status: Active\n‚úÖ Queue URL: Automatically generated (will be used in next step)\nSQS Queue Created! Store_Queue has been successfully created and is ready to receive messages from the system.\nQueue URL and Integration The Queue URL will have the format:\nhttps://sqs.ap-southeast-1.amazonaws.com/[YOUR-ACCOUNT-ID]/Store_Queue Note Queue URL: You will need to copy this Queue URL to configure in the Lambda function in the next step. This URL is the unique identifier for your queue.\nPreparing for Next Step Store_Queue is now ready to:\nReceive messages from data sources Trigger Lambda function when new messages arrive Ensure reliability in message processing Next Step: In the next step, we will create the \u0026ldquo;StorePlanningApp\u0026rdquo; Lambda function that will be triggered by messages from this Store_Queue.\nSummary Amazon SQS Store_Queue has been successfully set up with:\nReliable message queuing for store planning system Standard queue type for high throughput Ready for integration with Lambda function Scalable architecture supporting future growth "
},
{
	"uri": "//localhost:1313/3-createfirehosestream/",
	"title": "Data Firehose Stream",
	"tags": [],
	"description": "",
	"content": "In this module, you will create a Kinesis Data Firehose delivery stream - the core component of your streaming analytics pipeline. Firehose will collect inventory data from StoreApp and automatically write it to the S3 consumption bucket with optimized format for analytics.\nKinesis Data Firehose Overview Amazon Kinesis Data Firehose is a fully-managed service that helps you:\nAutomatically collect and transform streaming data from multiple sources Write data to data lake with optimized format (Parquet) for analytics Dynamic Partitioning to optimize Athena query performance Format conversion from JSON to Parquet automatically Retry and error handling to ensure data reliability Architecture Integration In our pipeline:\nStoreApp ‚Üí streams inventory data ‚Üí Firehose Firehose ‚Üí converts JSON to Parquet ‚Üí S3 Bucket Firehose ‚Üí updates metadata ‚Üí Glue Data Catalog Athena ‚Üí queries optimized data ‚Üí Analytics insights Step-by-Step Instructions Step 1: Navigate to Kinesis Console Go to AWS Console search bar Search \u0026ldquo;Kinesis\u0026rdquo; Click on \u0026ldquo;Kinesis\u0026rdquo; Step 2: Access Data Firehose In the \u0026ldquo;Get started\u0026rdquo; interface, select \u0026ldquo;Amazon Data Firehose\u0026rdquo; Click \u0026ldquo;Create Firehose stream\u0026rdquo; to start creating a firehose stream Step 3: Configure Source and Destination Source: Select \u0026ldquo;Direct PUT\u0026rdquo; Destination: Select \u0026ldquo;Amazon S3\u0026rdquo; Firehose stream name: Name it SI-Firehose Direct PUT: Means applications will send data directly to the Firehose stream instead of through an intermediate Kinesis Data Stream.\nStep 4: Enable Record Format Conversion Under \u0026ldquo;Transform and convert records\u0026rdquo; ‚Üí \u0026ldquo;Convert record format\u0026rdquo; Enable \u0026ldquo;Enable record format conversion\u0026rdquo; After enabling, additional details will appear: AWS Glue Region: Select the same region you\u0026rsquo;re using (Singapore) AWS Glue Database: Select conversion_db AWS Glue Table: Select conversion_table Format Conversion: Converting from JSON to Parquet reduces file size by up to 80% and significantly speeds up Athena queries.\nStep 5: Configure S3 Destination and Dynamic Partitioning Destination Settings ‚Üí S3 bucket: Select the consumption bucket created in section 2.1 Dynamic partitioning: Enable \u0026ldquo;Enabled\u0026rdquo; Inline parsing for JSON: Enable \u0026ldquo;Enabled\u0026rdquo; Step 6: Add Dynamic Partitioning Keys In the \u0026ldquo;Dynamic partitioning keys\u0026rdquo; section, add the keys as shown in the image After adding the keys, the \u0026ldquo;S3 bucket prefix\u0026rdquo; will be automatically filled To ensure accuracy, copy and paste the following into S3 bucket prefix: store-data/store_id=!{partitionKeyFromQuery:store_id}/!{partitionKeyFromQuery:year}/!{partitionKeyFromQuery:month}/!{partitionKeyFromQuery:day}/!{partitionKeyFromQuery:hour}/ S3 bucket error output prefix: Enter error Retry duration: Enter 60 seconds Dynamic Partitioning: Automatically organizes data by store_id and timestamp, helping Athena query faster by only scanning necessary partitions.\nStep 7: Configure IAM Role and Create Stream Expand the \u0026ldquo;Advanced settings\u0026rdquo; section Service access: Select \u0026ldquo;Choose existing IAM role\u0026rdquo; Select the \u0026ldquo;FirehoseWorkshop\u0026rdquo; IAM role created in the previous step Scroll down and click \u0026ldquo;Create firehose stream\u0026rdquo; Step 8: Verification After completing the setup, you should see the Firehose stream SI-Firehose successfully created with status \u0026ldquo;Active\u0026rdquo;.\nVerification Checklist Confirm the following configurations are set correctly:\n‚úÖ Source: Direct put\n‚úÖ Destination: Amazon S3 (your consumption bucket)\n‚úÖ Format conversion: Enabled (JSON ‚Üí Parquet)\n‚úÖ Glue integration: conversion_db.conversion_table\n‚úÖ Dynamic partitioning: Enabled with appropriate keys\n‚úÖ IAM role: FirehoseWorkshop\n‚úÖ Status: Active\nComplete! Kinesis Data Firehose stream is now ready to receive streaming data from StoreApp and automatically write to the data lake with optimized format.\nCost Note: Active Firehose stream will incur costs based on the amount of data processed. Remember to cleanup resources after completing the workshop.\n"
},
{
	"uri": "//localhost:1313/4-lambdafunctions/",
	"title": " Lambda Functions",
	"tags": [],
	"description": "",
	"content": "In this section, you will learn about AWS Lambda Functions and their role in the streaming analytics architecture. Lambda will play a crucial role in processing real-time data from stores and sending notifications to the delivery truck management system.\nAWS Lambda Overview AWS Lambda is a serverless computing service that lets you run code without provisioning or managing servers. Lambda automatically scales your application by running code in response to each trigger, automatically managing the underlying compute resources.\nWhy Use Lambda in Streaming Data Pipeline? In the context of our store inventory management system, Lambda serves as a real-time data processor:\nData Generator: Creates simulated inventory data from multiple different stores Streaming Integration: Sends data directly to Kinesis Data Firehose Event-Driven: Can be triggered manually or on a schedule JSON Format: Generates data with JSON structure compatible with Glue table schema StoreApp Lambda Function Features The StoreApp Lambda function will simulate retail stores sending inventory information such as:\n‚úÖ Store ID and Location: Store identification and location details\n‚úÖ Product Information: ID, name, and product category\n‚úÖ Stock Levels: Current inventory vs minimum threshold comparison\n‚úÖ Timestamps: Time stamps for real-time tracking\nLambda in Streaming Analytics: Lambda function serves as a crucial intermediary, receiving data from sources and transforming it before sending to Kinesis Data Firehose.\nLambda Advantages in Our System ‚ö° Serverless Computing No infrastructure management required AWS handles all provisioning and scaling Focus on business logic instead of server management üìà Auto-scaling Automatically scales with traffic Handles from few requests to thousands of requests/second No capacity planning configuration needed üí∞ Cost-effective Pay only when code executes No charges when idle Pricing model based on requests and compute time üîÑ Event-driven Responds immediately to events Integrates with multiple AWS services Supports real-time data processing Important Note: The Lambda function in this workshop will be triggered manually so you can observe and understand each step of the data processing pipeline.\nSummary AWS Lambda is the core component in our streaming analytics architecture. It provides:\nReal-time processing capability for store data Seamless integration with Kinesis Data Firehose Serverless architecture that reduces complexity Cost optimization with pay-per-use model Let\u0026rsquo;s start by creating the StoreApp Lambda Function in the next step!\n"
},
{
	"uri": "//localhost:1313/4-lambdafunctions/4.4-storeplanningapp/",
	"title": "StorePlanningApp  Function",
	"tags": [],
	"description": "",
	"content": "In this step, you will create the StorePlanningApp Lambda Function - an application that processes planning logic for the store system. This function will receive messages from the SQS Queue, perform data analysis from Athena, and generate supply planning strategies.\nStorePlanningApp Lambda Function Overview StorePlanningApp plays a crucial role in the analytics architecture:\nMessage Consumer: Receives and processes messages from SQS Store_Queue Data Analytics: Queries data from Athena/Glue for inventory analysis Planning Logic: Calculates demand and creates supply plans Result Storage: Stores analysis results in S3 bucket Step-by-Step Instructions Step 1: Navigate to Lambda Console In the AWS Console search bar Type \u0026ldquo;lambda\u0026rdquo; Select \u0026ldquo;Lambda\u0026rdquo; from the search results Step 2: Create New Function Click the \u0026ldquo;Create function\u0026rdquo; button Function name: Name it StorePlanningApp Runtime: Select \u0026ldquo;Python 3.13\u0026rdquo; Leave other configurations as default Click \u0026ldquo;Create function\u0026rdquo; StorePlanningApp: This function will contain more complex logic than StoreApp, including Athena queries and planning algorithms.\nStep 3: Deploy Function Code In the Lambda code interface of \u0026ldquo;StorePlanningApp\u0026rdquo; Perform the same steps as done with \u0026ldquo;StoreApp\u0026rdquo;: Create function.py file Configure handler to function.lambda_handler Copy and paste code from this link: üìÅ planning-function.py Click \u0026ldquo;Deploy\u0026rdquo; to deploy the code üìÅ Source Code Required: Copy content from link planning-function.py and paste into the function.py file in the Lambda editor.\nStep 4: Configure Environment Variables Navigate to \u0026ldquo;Configuration\u0026rdquo; section Select \u0026ldquo;Environment variables\u0026rdquo; Click \u0026ldquo;Edit\u0026rdquo; Add the following 4 environment variables:\nKey Value Description glue_db conversion_db Glue database name glue_table conversion_table Glue table name output_bucket s3://consumption-bucket-yourname/ S3 bucket output queue_url https://sqs.ap-southeast-1.amazonaws.com/yourID/Store_Queue SQS Queue URL Replace Information:\nReplace yourname in bucket name with your name Replace yourID in queue URL with your AWS Account ID Step 5: Configure Timeout Still in the \u0026ldquo;Configuration\u0026rdquo; section Select \u0026ldquo;General configuration\u0026rdquo; Click \u0026ldquo;Edit\u0026rdquo; Set Timeout to \u0026ldquo;2 min\u0026rdquo; (120 seconds) Click \u0026ldquo;Save\u0026rdquo; to save configuration Why 2 minutes timeout?: StorePlanningApp needs time to query Athena and process data, longer timeout ensures function doesn\u0026rsquo;t terminate early.\nStep 6: Configure IAM Permissions Navigate to IAM Console In the search bar, type \u0026ldquo;store\u0026rdquo; Find and select role \u0026ldquo;StorePlanningApp-role-xxxxx\u0026rdquo; Click \u0026ldquo;Add permissions\u0026rdquo; ‚Üí \u0026ldquo;Attach policies\u0026rdquo; Add the following policies (search and attach each policy): ‚úÖ AmazonAthenaFullAccess ‚úÖ AmazonS3FullAccess ‚úÖ AmazonSQSFullAccess ‚úÖ AWSGlueConsoleFullAccess IAM Permissions Explained:\nAthena: Query data from data lake S3: Read/write result files SQS: Receive messages from queue Glue: Access metadata catalog Step 7: Test Lambda Function Return to Lambda \u0026ldquo;StorePlanningApp\u0026rdquo; Navigate to \u0026ldquo;Test\u0026rdquo; section Create test event named \u0026ldquo;test\u0026rdquo; Click \u0026ldquo;Test\u0026rdquo; button to execute Step 8: Verify Test Results After successful test, you should see:\n‚úÖ Execution result: succeeded ‚úÖ Function logs: Detailed processing information ‚úÖ Duration: Execution time ‚úÖ Memory used: Resource consumption Verify Function Operation After completing the above steps, StorePlanningApp Lambda function has:\n‚úÖ Runtime: Python 3.13\n‚úÖ Handler: function.lambda_handler\n‚úÖ Source Code: Planning logic deployed\n‚úÖ Environment Variables: 4 variables configured\n‚úÖ Timeout: 2 minutes\n‚úÖ IAM Permissions: 4 policies attached\n‚úÖ Test Status: Successfully executed\nStorePlanningApp Complete! Lambda function has been created, configured, and tested successfully. Function is ready to process planning logic from SQS messages.\nSummary StorePlanningApp Lambda function now:\nReceives and processes messages from SQS Queue Queries data from Athena for analytics Executes planning logic based on inventory data Stores results in S3 bucket Ready to scale with message volume This function is the core component in our store planning and analytics pipeline!\n"
},
{
	"uri": "//localhost:1313/4-lambdafunctions/4.5-storetruckapp/",
	"title": "Create StoreTruckApp and Complete Pipeline",
	"tags": [],
	"description": "",
	"content": "In this final step, we will create the StoreTruckApp Lambda Function and Truck_Queue to complete the entire streaming analytics pipeline. StoreTruckApp will handle truck information and coordinate deliveries based on analysis results from StorePlanningApp.\nStoreTruckApp Overview StoreTruckApp is the final component in the streaming analytics architecture:\nTruck Management: Manage truck Delivery Coordination: Coordinate deliveries based on planning results Route Optimization: Optimize delivery routes Status Tracking: Track delivery status in real-time Step-by-Step Instructions Step 1: Create StoreTruckApp Lambda Function Go to Lambda Console Click \u0026ldquo;Create function\u0026rdquo; Configure function with same settings as previous 2 functions: Function name: StoreTruckApp Runtime: Python 3.13 Handler: function.lambda_handler Click \u0026ldquo;Create function\u0026rdquo; StoreTruckApp Purpose: This function will receive delivery demand information and coordinate delivery trucks optimally.\nStep 2: Deploy Source Code In the StoreTruckApp code interface Create function.py file Copy and paste code from this link: üìÅ truck-app-function.py Click \u0026ldquo;Deploy\u0026rdquo; to deploy code üìÅ Source Code Required: Copy content from link truck-app-function.py and paste into the function.py file.\nStep 3: Configure IAM Permissions Navigate to IAM Console In the search bar, find the \u0026ldquo;StoreTruckApp\u0026rdquo; role Select role \u0026ldquo;StoreTruckApp-role-xxxxx\u0026rdquo; Add permissions ‚Üí \u0026ldquo;Attach policies\u0026rdquo; Attach the following policies: ‚úÖ AmazonAthenaFullAccess ‚úÖ AmazonS3FullAccess ‚úÖ AmazonSQSFullAccess ‚úÖ AWSGlueConsoleFullAccess Same Permissions: StoreTruckApp needs the same permissions as StorePlanningApp to query data and send messages.\nStep 4: Configure Timeout Return to Lambda \u0026ldquo;StoreTruckApp\u0026rdquo; Go to \u0026ldquo;Configuration\u0026rdquo; ‚Üí \u0026ldquo;General configuration\u0026rdquo; Click \u0026ldquo;Edit\u0026rdquo; Set Timeout to \u0026ldquo;2 min\u0026rdquo; (120 seconds) Click \u0026ldquo;Save\u0026rdquo; to save Step 5: Create Truck_Queue Navigate to Amazon SQS Console Click \u0026ldquo;Create queue\u0026rdquo; Create queue with same configuration as before: Queue name: Truck_Queue Queue type: Standard Keep other settings as default Click \u0026ldquo;Create queue\u0026rdquo; Truck_Queue Role: This queue will receive delivery request messages and trigger StoreTruckApp for processing.\nStep 6: Configure Environment Variables Return to Lambda \u0026ldquo;StoreTruckApp\u0026rdquo; Navigate to \u0026ldquo;Configuration\u0026rdquo; ‚Üí \u0026ldquo;Environment variables\u0026rdquo; Click \u0026ldquo;Edit\u0026rdquo; Add environment variable: Key: queue_url Value: https://sqs.ap-southeast-1.amazonaws.com/yourID/Truck_Queue Replace yourID: Remember to replace yourID with your actual AWS Account ID in the Queue URL.\nStep 7: Test StoreTruckApp Function Go to \u0026ldquo;Test\u0026rdquo; section of StoreTruckApp Create test event named \u0026ldquo;test\u0026rdquo; Click \u0026ldquo;Test\u0026rdquo; button to execute Verify Complete Pipeline After completing all steps, the streaming analytics system has:\n‚úÖ Data Ingestion: StoreApp + Kinesis Firehose + S3\n‚úÖ Data Cataloging: AWS Glue Database \u0026amp; Table\n‚úÖ Data Analytics: Athena queries from StorePlanningApp\n‚úÖ Message Queuing: Store_Queue + Truck_Queue\n‚úÖ Business Logic: StorePlanningApp + StoreTruckApp\n‚úÖ End-to-End Flow: From stores to delivery trucks\nüéâ Workshop Complete! You have successfully completed the Streaming Ingestion \u0026amp; Analytics workshop with a complete serverless architecture on AWS!\nWorkshop Summary What we have built: üè™ Real-time Data Pipeline: Collect real-time data from stores\nüìä Serverless Analytics: Process and analyze data without server management\nüöõ Intelligent Routing: Coordinate delivery trucks based on data analysis\n‚ö° Event-driven Architecture: Entire system operates on events\nüí∞ Cost Optimized: Pay only when data processing occurs\nAWS Services Used: AWS Lambda: Serverless compute for business logic Amazon Kinesis Data Firehose: Streaming data ingestion Amazon S3: Data lake storage AWS Glue: Data cataloging and ETL Amazon Athena: Serverless analytics Amazon SQS: Message queuing IAM: Security and permissions Knowledge Gained: ‚úÖ Streaming Data Architecture design patterns\n‚úÖ Serverless Computing with AWS Lambda\n‚úÖ Real-time Analytics with Kinesis and Athena\n‚úÖ Event-driven Systems with SQS\n‚úÖ Data Lake Architecture with S3 and Glue\n‚úÖ Security Best Practices with IAM\nNext Steps Now you can:\nExtend Pipeline: Add more Lambda functions for advanced analytics Add Monitoring: Use CloudWatch to monitor the pipeline Implement Dashboard: Create visualization with QuickSight Scale Up: Apply this pattern for production workloads Cost Optimization: Fine-tune resources for cost efficiency Continue Learning: Try experimenting with other AWS services like EventBridge, Step Functions, or Elasticsearch to extend this pipeline!\nüéØ Congratulations! You have mastered building a complete streaming analytics pipeline on AWS!\n"
},
{
	"uri": "//localhost:1313/5-cleanup/",
	"title": "Resource Cleanup",
	"tags": [],
	"description": "",
	"content": "In this module, you will clean up all AWS resources created during the workshop to avoid unexpected costs. The cleanup will be performed in reverse order of creation to ensure no dependency errors.\nWhy Resource Cleanup is Important Cost Savings: Services like Firehose, Lambda, and S3 storage continue to incur charges if not deleted Resource Management: Avoid cluttering your AWS account with unused resources Security: Remove buckets and functions to avoid unnecessary security risks Cleanup Order Amazon S3 - Delete consumption bucket Amazon SQS - Delete notification queues AWS Glue - Delete database and tables AWS Lambda - Delete functions Step 1: Clean Up Amazon S3 Bucket Step 1.1: Access S3 Console Open AWS Console and search for \u0026ldquo;S3\u0026rdquo; Click \u0026ldquo;S3\u0026rdquo; to access S3 console Find and select the consumption bucket you created at the beginning of the workshop Click \u0026ldquo;Delete\u0026rdquo; Step 1.2: Empty Bucket Before Deletion In the \u0026ldquo;Delete bucket\u0026rdquo; interface, you\u0026rsquo;ll see a message that the bucket must be emptied first Click \u0026ldquo;Empty bucket\u0026rdquo; to delete all objects in the bucket Step 1.3: Confirm Empty Bucket In the \u0026ldquo;Empty bucket\u0026rdquo; dialog, type \u0026ldquo;permanently delete\u0026rdquo; to confirm Click \u0026ldquo;Empty\u0026rdquo; to execute the deletion of all objects Warning: Emptying the bucket will permanently delete all data. Ensure you have backed up important data if needed.\nStep 1.4: Delete Bucket After the bucket has been successfully emptied, return to the delete bucket interface Type the exact name of the S3 bucket to confirm Click \u0026ldquo;Delete bucket\u0026rdquo; to complete the deletion Step 2: Clean Up Amazon SQS Queues Step 2.1: Access SQS Console Open AWS Console and search for \u0026ldquo;SQS\u0026rdquo; Click \u0026ldquo;SQS\u0026rdquo; to access SQS console Select each Queue you created in the workshop Click \u0026ldquo;Delete\u0026rdquo; Step 2.2: Confirm Delete Queue In the confirmation dialog, type \u0026ldquo;delete\u0026rdquo; to confirm Click \u0026ldquo;Delete\u0026rdquo; to delete the queue Repeat this process for all created queues Note: You can select multiple queues at once for faster deletion by holding Ctrl and clicking on the queues.\nStep 3: Clean Up AWS Glue Database Step 3.1: Access Glue Console Open AWS Console and search for \u0026ldquo;Glue\u0026rdquo; Click \u0026ldquo;AWS Glue\u0026rdquo; to access Glue console In the left sidebar, select \u0026ldquo;Databases\u0026rdquo; Find and select \u0026ldquo;conversion_db\u0026rdquo; Click \u0026ldquo;Delete\u0026rdquo; Step 3.2: Confirm Delete Database In the confirmation dialog, read the message about deleting the database carefully Click \u0026ldquo;Delete\u0026rdquo; to confirm database deletion Information: Deleting the database will automatically delete all tables within it, including the conversion_table we created.\nStep 4: Clean Up AWS Lambda Functions Step 4.1: Access Lambda Console Open AWS Console and search for \u0026ldquo;Lambda\u0026rdquo; Click \u0026ldquo;Lambda\u0026rdquo; to access Lambda console Select all Lambda functions you created in the workshop Click \u0026ldquo;Actions\u0026rdquo; ‚Üí \u0026ldquo;Delete\u0026rdquo; Step 4.2: Confirm Delete Functions In the confirmation dialog, carefully read the list of functions to be deleted Type \u0026ldquo;delete\u0026rdquo; to confirm Click \u0026ldquo;Delete\u0026rdquo; to complete the deletion Warning: Deleting Lambda functions is irreversible. Ensure you have backed up the source code if you need to use it again.\nAdditional Resource Cleanup Kinesis Data Firehose If the Firehose stream is still running:\nAccess Kinesis Console Select \u0026ldquo;Data Firehose\u0026rdquo; Select \u0026ldquo;SI-Firehose\u0026rdquo; stream Click \u0026ldquo;Delete\u0026rdquo; and confirm IAM Roles (Optional) For complete cleanup:\nAccess IAM Console Select \u0026ldquo;Roles\u0026rdquo; Find all the roles we created Delete roles if not needed for other purposes Verification Checklist Verify that all resources have been successfully deleted:\n‚úÖ S3 Bucket: Consumption bucket has been deleted\n‚úÖ SQS Queues: All notification queues have been deleted\n‚úÖ Glue Database: conversion_db and conversion_table have been deleted\n‚úÖ Lambda Functions: All functions have been deleted\n‚úÖ Firehose Stream: SI-Firehose stream has been deleted (if applicable)\n‚úÖ IAM Roles: FirehoseWorkshop role has been deleted (optional)\nComplete! All AWS workshop resources have been successfully cleaned up. You will not be charged for these resources anymore.\nFinal Note: It may take a few minutes for AWS to complete the deletion of all resources. You can check the billing dashboard after 24 hours to ensure no charges are incurred.\n"
},
{
	"uri": "//localhost:1313/categories/",
	"title": "Categories",
	"tags": [],
	"description": "",
	"content": ""
},
{
	"uri": "//localhost:1313/tags/",
	"title": "Tags",
	"tags": [],
	"description": "",
	"content": ""
}]